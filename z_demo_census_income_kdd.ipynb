{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for the Fair-Shapely experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                                           0\n",
      "workclass                                     0\n",
      "industry_code                                 0\n",
      "occupation_code                               0\n",
      "education                                     0\n",
      "wage_per_hour                                 0\n",
      "enrolled_in_edu_inst_last_wk                  0\n",
      "marital_status                                0\n",
      "major_industry_code                           0\n",
      "major_occupation_code                         0\n",
      "race                                          0\n",
      "hispanic_origin                               0\n",
      "sex                                           0\n",
      "member_of_a_labour_union                      0\n",
      "reason_for_unemployment                       0\n",
      "employment_status                             0\n",
      "capital_gains                                 0\n",
      "capital_losses                                0\n",
      "dividend_from_stocks                          0\n",
      "tax_filler_status                             0\n",
      "region_of_previous_residence                  0\n",
      "state_of_previous_residence                   0\n",
      "detailed_household_and_family_stat            0\n",
      "detailed_household_summary_in_household       0\n",
      "instance_weight                               0\n",
      "migration_code_change_in_msa                  0\n",
      "migration_code_change_in_reg                  0\n",
      "migration_code_move_within_reg                0\n",
      "live_in_this_house_1_year_ag                  0\n",
      "migration_prev_res_in_sunbelt                 0\n",
      "num_persons_worked_for_employer               0\n",
      "family_members_under_18                       0\n",
      "country_of_birth_father                       0\n",
      "country_of_birth_mother                       0\n",
      "country_of_birth_self                         0\n",
      "citizenship                                   0\n",
      "own_business_or_self_employed                 0\n",
      "fill_inc_questionnaire_for_veteran's_admin    0\n",
      "veterans_benefits                             0\n",
      "weeks_worked_in_year                          0\n",
      "year                                          0\n",
      "class                                         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>capital_gains</th>\n",
       "      <th>capital_losses</th>\n",
       "      <th>dividend_from_stocks</th>\n",
       "      <th>instance_weight</th>\n",
       "      <th>num_persons_worked_for_employer</th>\n",
       "      <th>weeks_worked_in_year</th>\n",
       "      <th>sex</th>\n",
       "      <th>workclass_ Federal government</th>\n",
       "      <th>...</th>\n",
       "      <th>own_business_or_self_employed_2</th>\n",
       "      <th>fill_inc_questionnaire_for_veteran's_admin_ No</th>\n",
       "      <th>fill_inc_questionnaire_for_veteran's_admin_ Not in universe</th>\n",
       "      <th>fill_inc_questionnaire_for_veteran's_admin_ Yes</th>\n",
       "      <th>veterans_benefits_0</th>\n",
       "      <th>veterans_benefits_1</th>\n",
       "      <th>veterans_benefits_2</th>\n",
       "      <th>year_94</th>\n",
       "      <th>year_95</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.706939</td>\n",
       "      <td>-0.203989</td>\n",
       "      <td>-0.093502</td>\n",
       "      <td>-0.138177</td>\n",
       "      <td>-0.10222</td>\n",
       "      <td>-0.043817</td>\n",
       "      <td>-0.843184</td>\n",
       "      <td>-0.970621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.030500</td>\n",
       "      <td>-0.203989</td>\n",
       "      <td>-0.093502</td>\n",
       "      <td>-0.138177</td>\n",
       "      <td>-0.10222</td>\n",
       "      <td>-0.691534</td>\n",
       "      <td>-0.421771</td>\n",
       "      <td>1.158280</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.773339</td>\n",
       "      <td>-0.203989</td>\n",
       "      <td>-0.093502</td>\n",
       "      <td>-0.138177</td>\n",
       "      <td>-0.10222</td>\n",
       "      <td>-0.753246</td>\n",
       "      <td>-0.843184</td>\n",
       "      <td>-0.970621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  wage_per_hour  capital_gains  capital_losses  \\\n",
       "0  1.706939      -0.203989      -0.093502       -0.138177   \n",
       "1  1.030500      -0.203989      -0.093502       -0.138177   \n",
       "2 -0.773339      -0.203989      -0.093502       -0.138177   \n",
       "\n",
       "   dividend_from_stocks  instance_weight  num_persons_worked_for_employer  \\\n",
       "0              -0.10222        -0.043817                        -0.843184   \n",
       "1              -0.10222        -0.691534                        -0.421771   \n",
       "2              -0.10222        -0.753246                        -0.843184   \n",
       "\n",
       "   weeks_worked_in_year  sex  workclass_ Federal government  ...  \\\n",
       "0             -0.970621  0.0                            0.0  ...   \n",
       "1              1.158280  1.0                            0.0  ...   \n",
       "2             -0.970621  0.0                            0.0  ...   \n",
       "\n",
       "   own_business_or_self_employed_2  \\\n",
       "0                              0.0   \n",
       "1                              0.0   \n",
       "2                              0.0   \n",
       "\n",
       "   fill_inc_questionnaire_for_veteran's_admin_ No  \\\n",
       "0                                             0.0   \n",
       "1                                             0.0   \n",
       "2                                             0.0   \n",
       "\n",
       "   fill_inc_questionnaire_for_veteran's_admin_ Not in universe  \\\n",
       "0                                                1.0             \n",
       "1                                                1.0             \n",
       "2                                                1.0             \n",
       "\n",
       "   fill_inc_questionnaire_for_veteran's_admin_ Yes  veterans_benefits_0  \\\n",
       "0                                              0.0                  0.0   \n",
       "1                                              0.0                  0.0   \n",
       "2                                              0.0                  0.0   \n",
       "\n",
       "   veterans_benefits_1  veterans_benefits_2  year_94  year_95  class  \n",
       "0                  0.0                  1.0      0.0      1.0      1  \n",
       "1                  0.0                  1.0      1.0      0.0      1  \n",
       "2                  0.0                  1.0      0.0      1.0      1  \n",
       "\n",
       "[3 rows x 506 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.unified_dataloader import load_dataset\n",
    "\n",
    "_, processed_census_income_kdd = load_dataset('census_income_kdd')\n",
    "processed_census_income_kdd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split label/unlabel data, split train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_label shape: (8582, 505)\n",
      "X_unlabel shape: (77244, 505)\n",
      "---------------------------------\n",
      "X_train shape: (6007, 505)\n",
      "X_test shape: (2575, 505)\n"
     ]
    }
   ],
   "source": [
    "'''Census income kdd dataset'''\n",
    "df = processed_census_income_kdd.copy()\n",
    "# 随机抽取 40% 的数据 ------ 因为数据量实在太大了，计算太慢了\n",
    "sampled_df = df.sample(frac=0.3, random_state=42)\n",
    "# 重置索引\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n",
    "X = sampled_df.drop('class', axis=1)\n",
    "y = sampled_df['class']\n",
    "\n",
    "\n",
    "\n",
    "# 10% X_label, 90% X_unlabel\n",
    "X_label, X_unlabel, y_label, y_unlabel = train_test_split(X, y, test_size=0.9, random_state=25) \n",
    "print(f'X_label shape: {X_label.shape}')\n",
    "print(f'X_unlabel shape: {X_unlabel.shape}')\n",
    "print('---------------------------------')\n",
    "# split labeled data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_label, y_label, test_size=0.3, random_state=25) \n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9363106796116505\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()  # 可以替换为 RandomForestClassifier() 等其他模型\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# 预测和评估\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-10 18:50:35.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline1\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mbaseline1: 使用了xgboost, Accuracy: 0.936, DR: 0.08188\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.baselines import Baseline\n",
    "\n",
    "baseline = Baseline(X_train, y_train, X_test, y_test, X_unlabel, model='xgboost')\n",
    "baseline.get_baseline1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-10 18:50:38.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.2, num_new_data: 1, Accuracy: 0.938, DR: 0.06387\u001b[0m\n",
      "\u001b[32m2025-01-10 18:50:43.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.2, num_new_data: 2, Accuracy: 0.937, DR: 0.06044\u001b[0m\n",
      "\u001b[32m2025-01-10 18:50:50.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.2, num_new_data: 3, Accuracy: 0.939, DR: 0.05629\u001b[0m\n",
      "\u001b[32m2025-01-10 18:50:55.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.4, num_new_data: 1, Accuracy: 0.938, DR: 0.06697\u001b[0m\n",
      "\u001b[32m2025-01-10 18:51:02.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.4, num_new_data: 2, Accuracy: 0.936, DR: 0.06085\u001b[0m\n",
      "\u001b[32m2025-01-10 18:51:14.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.4, num_new_data: 3, Accuracy: 0.937, DR: 0.05792\u001b[0m\n",
      "\u001b[32m2025-01-10 18:51:21.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.6, num_new_data: 1, Accuracy: 0.938, DR: 0.06600\u001b[0m\n",
      "\u001b[32m2025-01-10 18:51:33.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.6, num_new_data: 2, Accuracy: 0.938, DR: 0.06408\u001b[0m\n",
      "\u001b[32m2025-01-10 18:51:54.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.6, num_new_data: 3, Accuracy: 0.936, DR: 0.05899\u001b[0m\n",
      "\u001b[32m2025-01-10 18:52:05.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.8, num_new_data: 1, Accuracy: 0.937, DR: 0.06619\u001b[0m\n",
      "\u001b[32m2025-01-10 18:52:30.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.8, num_new_data: 2, Accuracy: 0.938, DR: 0.06295\u001b[0m\n",
      "\u001b[32m2025-01-10 18:53:04.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline2\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mbaseline2: 使用了xgboost, proportion: 0.8, num_new_data: 3, Accuracy: 0.937, DR: 0.06052\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用nearest neighbour在unlabel data中挑选与X_train相近的数据, 他们label使用与之匹配的X_train的label, 然后加入training set.   (结果不行)\n",
    "'''\n",
    "baseline.get_baseline2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-10 18:53:09.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.2, num_new_data: 1, Accuracy: 0.939, DR: 0.04834\u001b[0m\n",
      "\u001b[32m2025-01-10 18:53:14.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.2, num_new_data: 2, Accuracy: 0.938, DR: 0.03439\u001b[0m\n",
      "\u001b[32m2025-01-10 18:53:21.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.2, num_new_data: 3, Accuracy: 0.938, DR: 0.02766\u001b[0m\n",
      "\u001b[32m2025-01-10 18:53:26.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.4, num_new_data: 1, Accuracy: 0.937, DR: 0.04778\u001b[0m\n",
      "\u001b[32m2025-01-10 18:53:35.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.4, num_new_data: 2, Accuracy: 0.938, DR: 0.03673\u001b[0m\n",
      "\u001b[32m2025-01-10 18:53:49.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.4, num_new_data: 3, Accuracy: 0.938, DR: 0.02997\u001b[0m\n",
      "\u001b[32m2025-01-10 18:53:56.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.6, num_new_data: 1, Accuracy: 0.938, DR: 0.04710\u001b[0m\n",
      "\u001b[32m2025-01-10 18:54:11.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.6, num_new_data: 2, Accuracy: 0.937, DR: 0.03567\u001b[0m\n",
      "\u001b[32m2025-01-10 18:54:32.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.6, num_new_data: 3, Accuracy: 0.937, DR: 0.02971\u001b[0m\n",
      "\u001b[32m2025-01-10 18:54:44.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.8, num_new_data: 1, Accuracy: 0.938, DR: 0.04673\u001b[0m\n",
      "\u001b[32m2025-01-10 18:55:04.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.8, num_new_data: 2, Accuracy: 0.937, DR: 0.03598\u001b[0m\n",
      "\u001b[32m2025-01-10 18:55:37.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.baselines\u001b[0m:\u001b[36mget_baseline3\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mbaseline3: 使用了xgboost, proportion: 0.8, num_new_data: 3, Accuracy: 0.939, DR: 0.02928\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用nearest neighbour在unlabel data中挑选与X_train相近的数据, 直接把挑选出来的数据输入进model, 预测结果作为新的label, 然后加入training set. （效果很好）\n",
    "'''\n",
    "baseline.get_baseline3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Start the experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. 只使用10%的labeled data 查看模型结果-baseline （见4, 5）\n",
    ">\n",
    ">2. 只使用10%的labeled data + sex balance查看模型结果\n",
    ">\n",
    ">3. 使用10%的labeled data， 再使用unlabel data把labeled data中的sex补齐。（使用model预测的值对unlabel data贴标签）\n",
    ">\n",
    ">4. 使用10%的labeled data， 再使用unlabel data时候，让新增的data中 sex balanced（使用model预测的值对unlabel data贴标签）\n",
    ">\n",
    ">5. 使用10%的labeled data + sex balance + new data (from unlabeled data) + sex balance + 使用labeled data中最相似的instance的标签\n",
    ">\n",
    ">6. 使用10%的labeled data + sex balance + new data (from unlabeled data) + sex balance + 通过pretrained model对data进行label标注\n",
    ">\n",
    ">7.☆☆  这里的sex balance是如何选取好点？ 是分别找sex=0和sex=1的nearest neighbour(此处数量怎么限制？按照少数的来？)。\n",
    ">   还是把他们放在一个大池子里。统一挑出来，最后再限制个数为min(num_sex0, num_sex1)， 如果这样的话，对unlabeled data标注方式感觉不能直接使用原标签，是否用ml预测的结果会更好一点？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n对于german credit，需要把 self.limited_values_range = np.arange(1, non_zero_count, 1)中的1改成50\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_experiment = Experiment(\n",
    "            orginal_model=model, \n",
    "            X_train=X_train, \n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            X_unlabel=X_unlabel,\n",
    "            y_unlabel=y_unlabel,\n",
    "            dataset_name='census_income_kdd', )\n",
    "'''\n",
    "对于german credit，需要把 self.limited_values_range = np.arange(1, non_zero_count, 1)中的1改成50\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第3步, 计算每组数据的fairness shapley value\n",
      "开始第4步, 对shapely value不取绝对值, 然后把负值直接变成0, 然后在归一化, 得到新的varphi\n",
      "开始第5步, 计算q\n",
      "开始第6步, 计算出总共可以修改的actions number, 并且把新的unlabel data整合好, 加入到X_train中, 返回合并后的数据, 重新训练模型, 并且评估性能\n",
      "Total number of non-zero values across all varphis: 41818\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# new_models_DR_values = my_experiment.get_result(\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#             sex_balance = False, \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#             proportion = 0.5,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#             replacement = True, \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#             num_new_data = 3,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#             matcher = 'nn')\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m new_models_DR_values,_ \u001b[38;5;241m=\u001b[39m \u001b[43mmy_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sex_separate_nn_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msex_balance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproportion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_new_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msex_separate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Github-desktop\\Unlabeled-Fairness\\src\\experiments.py:318\u001b[0m, in \u001b[0;36mExperiment.get_sex_separate_nn_result\u001b[1;34m(self, sex_balance, proportion, replacement, num_new_data, matcher, match_method)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# Step 4: 替换 X 中前三列的值为 S 中对应位置的值\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value, row_idx, col_idx \u001b[38;5;129;01min\u001b[39;00m top_positions:\n\u001b[1;32m--> 318\u001b[0m     \u001b[43mx_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m q_combined[row_idx, col_idx]\n\u001b[0;32m    319\u001b[0m X_Train_New \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([x_train_with_target_sex0,x_train_with_target_sex1, x_copy], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Step 5: Train and evaluate model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ZhuLi\\anaconda3\\envs\\unlabel_fair\\lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZhuLi\\anaconda3\\envs\\unlabel_fair\\lib\\site-packages\\pandas\\core\\indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\ZhuLi\\anaconda3\\envs\\unlabel_fair\\lib\\site-packages\\pandas\\core\\indexing.py:2035\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m   2034\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[1;32m-> 2035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZhuLi\\anaconda3\\envs\\unlabel_fair\\lib\\site-packages\\pandas\\core\\indexing.py:2175\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mvoid:\n\u001b[0;32m   2166\u001b[0m         \u001b[38;5;66;03m# This means we're expanding, with multiple columns, e.g.\u001b[39;00m\n\u001b[0;32m   2167\u001b[0m         \u001b[38;5;66;03m#     df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]})\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[38;5;66;03m# Here, we replace those temporary `np.void` columns with\u001b[39;00m\n\u001b[0;32m   2171\u001b[0m         \u001b[38;5;66;03m# columns of the appropriate dtype, based on `value`.\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[:, loc] \u001b[38;5;241m=\u001b[39m construct_1d_array_from_inferred_fill_value(\n\u001b[0;32m   2173\u001b[0m             value, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   2174\u001b[0m         )\n\u001b[1;32m-> 2175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplane_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\ZhuLi\\anaconda3\\envs\\unlabel_fair\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1338\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[1;34m(self, loc, idx, value, inplace_only)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1337\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m col_mgr\u001b[38;5;241m.\u001b[39msetitem((idx,), value)\n\u001b[1;32m-> 1338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_to_warn:\n\u001b[0;32m   1341\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1342\u001b[0m         COW_WARNING_GENERAL_MSG,\n\u001b[0;32m   1343\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1344\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1345\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ZhuLi\\anaconda3\\envs\\unlabel_fair\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1148\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value, inplace, refs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iset_split_block(\n\u001b[0;32m   1145\u001b[0m             blkno_l, blk_locs, value_getitem(val_locs), refs\u001b[38;5;241m=\u001b[39mrefs\n\u001b[0;32m   1146\u001b[0m         )\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1148\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_locs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_locs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ZhuLi\\anaconda3\\envs\\unlabel_fair\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1285\u001b[0m, in \u001b[0;36mBlock.set_inplace\u001b[1;34m(self, locs, values, copy)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m-> 1285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlocs\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m values\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# new_models_DR_values = my_experiment.get_result(\n",
    "#             sex_balance = False, \n",
    "#             proportion = 0.5,\n",
    "#             replacement = True, \n",
    "#             num_new_data = 3,\n",
    "#             matcher = 'nn')\n",
    "\n",
    "new_models_DR_values,_ = my_experiment.get_sex_separate_nn_result(\n",
    "    sex_balance=False,\n",
    "    proportion=0.6,\n",
    "    replacement=True,\n",
    "    num_new_data=3,\n",
    "    matcher='nn',\n",
    "    match_method='sex_separate'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment.visualize(new_models_DR_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = my_experiment.combination(match_met='together')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sex_separate = my_experiment.combination(match_met='sex_separate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第3步, 计算每组数据的fairness shapley value\n"
     ]
    }
   ],
   "source": [
    "results_sex_cross = my_experiment.combination(match_met='sex_cross')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlabel_fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
